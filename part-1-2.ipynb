{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "extra-shadow",
   "metadata": {},
   "source": [
    "# VAE6\n",
    "## Changes to VAE3\n",
    "* Add robustness to blurriness\n",
    "* Improved beta-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-determination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pytorch3d\n",
    "from skimage.filters import gaussian\n",
    "\n",
    "import easydict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from vae_3d import VAE3D\n",
    "from gaussian_smoothing import GaussianSmoothing\n",
    "\n",
    "# Check whether GPU is available.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters.\n",
    "args = easydict.EasyDict({\n",
    "    'train': True,\n",
    "    'batch_size': 64,       # input batch size\n",
    "    'n_epochs': 500,         # number of epochs\n",
    "    'n_workers': 4,         # number of data loading workers\n",
    "    'learning_rate': 0.0005, # learning rate\n",
    "    'beta1': 0.9,           # beta 1\n",
    "    'beta2': 0.999,         # beta 2\n",
    "    'milestones': [1],        # step size\n",
    "    'gamma': 1,           # gamma\n",
    "    'beta': 1000,\n",
    "    'maxC': 5,\n",
    "    'bce_weight': 0.9,\n",
    "    'l2_reg': 0.0,\n",
    "    'sigma': 2,\n",
    "    'model': '',                            # model path\n",
    "    'out_dir': 'outputs_vae6'    # output directory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.shapenet_voxel import ShapeNetVoxel, ShapeNetVoxel32\n",
    "\n",
    "SYNSET_CHAIR = '03001627'\n",
    "SYNSET_JAR = '03593526'\n",
    "\n",
    "SHAPENET_PATH = '/home/ubuntu/voxel-autoencoder/shapenet/ShapeNetCore.v2'\n",
    "R2N2_PATH = '/home/ubuntu/voxel-autoencoder/shapenet/ShapeNetVox32'\n",
    "\n",
    "shapenet_dataset = ShapeNetVoxel32(R2N2_PATH, synsets=[SYNSET_CHAIR], version=2, load_textures=True)\n",
    "\n",
    "len(shapenet_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and create dataloaders.\n",
    "def create_datasets_and_dataloaders():\n",
    "    full_size = len(shapenet_dataset)\n",
    "    train_size = int(0.8 * full_size)\n",
    "    test_size = full_size - train_size\n",
    "    \n",
    "    train_data, test_data = torch.utils.data.random_split(shapenet_dataset, [train_size, test_size])\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=args.train,\n",
    "        num_workers=int(args.n_workers))\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=args.train,\n",
    "        num_workers=int(args.n_workers))\n",
    "\n",
    "    return train_data, train_dataloader, test_data, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cross entropy loss function.\n",
    "def weighted_binary_cross_entropy(output, target):\n",
    "    gamma = args.bce_weight\n",
    "    return -torch.mean(gamma*(target)*torch.log(output) + (1-gamma)*(1-target)*torch.log(1-output))\n",
    "\n",
    "def compute_loss(in_voxels, out_voxels, mean, logvar, epoch):\n",
    "    # in_voxels: (batch_size, 1, V, V, V)\n",
    "    # out_voxels: (batch_size, 1, V, V, V)\n",
    "    # mean: (batch_size, Z)\n",
    "    # logvar: (batch_size, Z)\n",
    "    recon_loss = weighted_binary_cross_entropy(0.1 + 0.8999 * out_voxels, -1 + 6*in_voxels)\n",
    "    kl_loss = 0.5 * torch.mean(torch.exp(logvar) + mean**2 - 1 - logvar)\n",
    "    if epoch is None:\n",
    "        epoch = args.n_epochs\n",
    "    C = args.maxC * np.clip((epoch - 100) / 300, 0, 1)\n",
    "    beta = -1 + ((args.beta+1) ** np.clip((epoch - 100) / 300, 0, 1))\n",
    "#     beta = args.beta * np.clip((epoch - 100) / 300, 0, 1)\n",
    "    beta = np.clip(beta, 0, args.beta)\n",
    "    \n",
    "    loss = recon_loss + beta*torch.abs(kl_loss - C)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the accuracy function.\n",
    "def compute_accuracy(in_voxels, out_voxels):\n",
    "    # in_voxels: (batch_size, 1, voxel_size, voxel_size, voxel_size)\n",
    "    # out_voxels: (batch_size, 1, voxel_size, voxel_size, voxel_size)\n",
    "    batch_size = in_voxels.shape[0]\n",
    "    in_bin = in_voxels > 0.5\n",
    "    out_bin = out_voxels > 0.5\n",
    "    acc = (in_bin & out_bin).reshape(batch_size, -1).sum(axis=1) / (in_bin | out_bin).reshape(batch_size, -1).sum(axis=1)\n",
    "    return acc.mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one-step training function.\n",
    "blur = torch.nn.Sequential(\n",
    "    torch.nn.ConstantPad3d(args.sigma, 0),\n",
    "    GaussianSmoothing(1, args.sigma * 2 + 1, args.sigma, 3)\n",
    ").cuda()\n",
    "\n",
    "def run_train(data, net, optimizer, writer=None, iter_epoch=None):\n",
    "    # Parse data.\n",
    "    original_voxels = data.cuda()\n",
    "    in_voxels = blur(original_voxels)\n",
    "    # in_voxels: (batch_size, 1, V, V, V)\n",
    "\n",
    "    # Reset gradients.\n",
    "    # https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html#zero-the-gradients-while-training-the-network\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Predict.\n",
    "    out_voxels, mean, logvar = net.train()(in_voxels)\n",
    "\n",
    "    # Compute the loss.\n",
    "    loss = compute_loss(original_voxels, out_voxels, mean, logvar, iter_epoch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Compute the accuracy.\n",
    "        acc = compute_accuracy(original_voxels, out_voxels)\n",
    "\n",
    "    # Backprop.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one-step evaluation function.\n",
    "def run_eval(data, net, optimizer, writer=None, iter_epoch=None):\n",
    "    # Parse data.\n",
    "    original_voxels = data.cuda()\n",
    "    in_voxels = blur(original_voxels)\n",
    "    # in_voxels: (batch_size, 1, V, V, V)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Predict.\n",
    "        out_voxels, mean, logvar = net.eval()(in_voxels)\n",
    "\n",
    "        # Compute the loss.\n",
    "        loss = compute_loss(original_voxels, out_voxels, mean, logvar, iter_epoch)\n",
    "\n",
    "        # Compute the accuracy.\n",
    "        acc = compute_accuracy(original_voxels, out_voxels)\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one-epoch training/evaluation function.\n",
    "def run_epoch(dataset, dataloader, train, epoch=None, writer=None):\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    n_data = len(dataset)\n",
    "\n",
    "    # Create a progress bar.\n",
    "    pbar = tqdm(total=n_data, leave=False)\n",
    "\n",
    "    mode = 'Train' if train else 'Test'\n",
    "    epoch_str = '' if epoch is None else '[Epoch {}/{}]'.format(\n",
    "            str(epoch).zfill(len(str(args.n_epochs))), args.n_epochs)\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # Run one step.\n",
    "        iter_epoch = args.n_epochs if epoch is None else epoch + i/len(dataloader)\n",
    "        loss, acc = run_train(data, net, optimizer, writer, iter_epoch) if train else \\\n",
    "                run_eval(data, net, optimizer, writer, iter_epoch)\n",
    "\n",
    "        if train and writer is not None:\n",
    "            # Write results if training.\n",
    "            assert(epoch is not None)\n",
    "            step = epoch * len(dataloader) + i\n",
    "            writer.add_scalar('Loss/Train', loss, step)\n",
    "            writer.add_scalar('Accuracy/Train', acc, step)\n",
    "\n",
    "        batch_size = data.shape[0]\n",
    "        total_loss += (loss * batch_size)\n",
    "        total_acc += (acc * batch_size)\n",
    "\n",
    "        pbar.set_description('{} {} Loss: {:f}, Acc : {:.2f}%'.format(\n",
    "            epoch_str, mode, loss, acc))\n",
    "        pbar.update(batch_size)\n",
    "\n",
    "    pbar.close()\n",
    "    mean_loss = total_loss / float(n_data)\n",
    "    mean_acc = total_acc / float(n_data)\n",
    "    return mean_loss, mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one-epoch function for both training and evaluation.\n",
    "def run_epoch_train_and_test(\n",
    "    train_dataset, train_dataloader, test_dataset, test_dataloader, epoch=None,\n",
    "        writer=None):\n",
    "    train_loss, train_acc = run_epoch(\n",
    "        train_dataset, train_dataloader, train=args.train, epoch=epoch,\n",
    "        writer=writer)\n",
    "    test_loss, test_acc = run_epoch(\n",
    "        test_dataset, test_dataloader, train=False, epoch=epoch, writer=None)\n",
    "\n",
    "    if writer is not None:\n",
    "        # Write test results.\n",
    "        assert(epoch is not None)\n",
    "        step = (epoch + 1) * len(train_dataloader)\n",
    "        writer.add_scalar('Loss/Test', test_loss, step)\n",
    "        writer.add_scalar('Accuracy/Test', test_acc, step)\n",
    "\n",
    "    epoch_str = '' if epoch is None else '[Epoch {}/{}]'.format(\n",
    "            str(epoch).zfill(len(str(args.n_epochs))), args.n_epochs)\n",
    "\n",
    "    log = epoch_str + ' '\n",
    "    log += 'Train Loss: {:f}, '.format(train_loss)\n",
    "    log += 'Train Acc: {:.2f}%, '.format(train_acc)\n",
    "    log += 'Test Loss: {:f}, '.format(test_loss)\n",
    "    log += 'Test Acc: {:.2f}%.'.format(test_acc)\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function.\n",
    "if __name__ == \"__main__\":\n",
    "    print(args)\n",
    "\n",
    "    # Load datasets.\n",
    "    train_dataset, train_dataloader, test_dataset, test_dataloader = create_datasets_and_dataloaders()\n",
    "\n",
    "    # Create the network.\n",
    "    net = VAE3D(32)\n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "\n",
    "    # Load a model if given.\n",
    "    if args.model != '':\n",
    "        net.load_state_dict(torch.load(args.model))\n",
    "\n",
    "    # Set an optimizer and a scheduler.\n",
    "#     optimizer = torch.optim.SGD(\n",
    "#         net.parameters(), lr=args.learning_rate,\n",
    "#         momentum=0.9, weight_decay=args.l2_reg, nesterov=True)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        net.parameters(), lr=args.learning_rate,\n",
    "        betas=(args.beta1, args.beta2), weight_decay=args.l2_reg)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=args.milestones, gamma=args.gamma)\n",
    "\n",
    "    # Create the output directory.\n",
    "    if not os.path.exists(args.out_dir):\n",
    "        os.makedirs(args.out_dir)\n",
    "\n",
    "    # Train.\n",
    "    if args.train:\n",
    "        writer = SummaryWriter(args.out_dir)\n",
    "\n",
    "        for epoch in range(args.n_epochs):\n",
    "            run_epoch_train_and_test(\n",
    "                train_dataset, train_dataloader, test_dataset, test_dataloader,\n",
    "                epoch, writer)\n",
    "\n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                # Save the model.\n",
    "                model_file = os.path.join(\n",
    "                    args.out_dir, 'model_{:d}.pth'.format(epoch + 1))\n",
    "                torch.save(net.state_dict(), model_file)\n",
    "                print(\"Saved '{}'.\".format(model_file))\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        writer.close()\n",
    "    else:\n",
    "        run_epoch_train_and_test(\n",
    "            train_dataset, train_dataloader, test_dataset, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function.\n",
    "if __name__ == \"__main__\":\n",
    "    print(args)\n",
    "\n",
    "    # Load datasets.\n",
    "    train_dataset, train_dataloader, test_dataset, test_dataloader = create_datasets_and_dataloaders()\n",
    "\n",
    "    # Create the network.\n",
    "#     net = VAE3D(100)\n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "\n",
    "    # Load a model if given.\n",
    "    if args.model != '':\n",
    "        net.load_state_dict(torch.load(args.model))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        import random\n",
    "        train_data = random.choice(test_dataset).unsqueeze(0)\n",
    "        train_out, _, _ = net.eval()(blur(train_data.cuda()))\n",
    "#         train_out, _, _ = net.eval()(train_data.cuda())\n",
    "    \n",
    "    sample_origin = train_data[0,0,:,:,:].cpu().numpy()\n",
    "    sample_recon = train_out[0,0,:,:,:].cpu().numpy()\n",
    "    \n",
    "    print(sample_recon.max(), sample_recon.min())\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    print('drawing original')\n",
    "    ax1 = plt.figure().add_subplot(projection='3d')\n",
    "    ax1.voxels(sample_origin > 0.5)\n",
    "    print('drawing reconstruction')\n",
    "    ax2 = plt.figure().add_subplot(projection='3d')\n",
    "    ax2.voxels(sample_recon > 0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function.\n",
    "if __name__ == \"__main__\":\n",
    "    print(args)\n",
    "\n",
    "    # Load datasets.\n",
    "    train_dataset, train_dataloader, test_dataset, test_dataloader = create_datasets_and_dataloaders()\n",
    "\n",
    "    # Create the network.\n",
    "#     net = VAE3D(128)\n",
    "#     if torch.cuda.is_available():\n",
    "#         net.cuda()\n",
    "\n",
    "#     # Load a model if given.\n",
    "#     args.model = 'outputs_vae5/model_200.pth'\n",
    "#     if args.model != '':\n",
    "#         net.load_state_dict(torch.load(args.model))\n",
    "\n",
    "    num_points = len(train_dataset) + len(test_dataset)\n",
    "    z = np.zeros((num_points, 32))\n",
    "    c = np.zeros((num_points))\n",
    "    with torch.no_grad():\n",
    "        idx = 0\n",
    "        for train_data in tqdm(train_dataset):\n",
    "            train_data = train_data.unsqueeze(0)\n",
    "            mean, _ = net.encoder.eval()(blur(train_data.cuda()))\n",
    "            z[idx,:] = mean[0,:].cpu().numpy()\n",
    "            c[idx] = 0\n",
    "            idx += 1\n",
    "        \n",
    "        for test_data in tqdm(test_dataset):\n",
    "            test_data = test_data.unsqueeze(0)\n",
    "            mean, _ = net.encoder.eval()(blur(test_data.cuda()))\n",
    "            z[idx,:] = mean[0,:].cpu().numpy()\n",
    "            c[idx] = 1\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "z_embedded = TSNE().fit_transform(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.scatter(z_embedded[:,0], z_embedded[:,1], c=c, cmap='tab10')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_loss(in_voxels, out_voxels, mean, logvar, epoch):\n",
    "    # in_voxels: (batch_size, 1, V, V, V)\n",
    "    # out_voxels: (batch_size, 1, V, V, V)\n",
    "    # mean: (batch_size, Z)\n",
    "    # logvar: (batch_size, Z)\n",
    "    kl_loss = 0.5 * torch.mean(torch.exp(logvar) + mean**2 - 1 - logvar)\n",
    "    return kl_loss\n",
    "\n",
    "# Main function.\n",
    "if __name__ == \"__main__\":\n",
    "    print(args)\n",
    "\n",
    "    # Load datasets.\n",
    "    train_dataset, train_dataloader, test_dataset, test_dataloader = create_datasets_and_dataloaders()\n",
    "\n",
    "    # Create the network.\n",
    "#     net = VAE3D(128)\n",
    "#     if torch.cuda.is_available():\n",
    "#         net.cuda()\n",
    "\n",
    "#     # Load a model if given.\n",
    "#     args.model = 'outputs_vae5/model_200.pth'\n",
    "#     if args.model != '':\n",
    "#         net.load_state_dict(torch.load(args.model))\n",
    "\n",
    "    num_points = len(train_dataset) + len(test_dataset)\n",
    "#     num_points = len(test_dataset)\n",
    "    loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        idx = 0\n",
    "        for train_data in tqdm(train_dataset):\n",
    "            train_data = train_data.unsqueeze(0)\n",
    "            mean, logvar = net.encoder.eval()(blur(train_data.cuda()))\n",
    "            loss += 0.5 * torch.mean(torch.exp(logvar) + mean**2 - 1 - logvar)\n",
    "        \n",
    "        for test_data in tqdm(test_dataset):\n",
    "            test_data = test_data.unsqueeze(0)\n",
    "            mean, logvar = net.encoder.eval()(blur(test_data.cuda()))\n",
    "            loss += 0.5 * torch.mean(torch.exp(logvar) + mean**2 - 1 - logvar)\n",
    "    loss /= num_points\n",
    "    print(f'KL Divergence = {loss:.5f} nats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _, _, pc_basis = torch.pca_lowrank(torch.tensor(z).unsqueeze(0).cuda(), 32)\n",
    "    model_file = os.path.join(\n",
    "        args.out_dir, 'model_basis.pth')\n",
    "    torch.save(pc_basis, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function.\n",
    "def visualize(data, threshold=0.5, desc=''):\n",
    "    import matplotlib.pyplot as plt\n",
    "    ax1 = plt.figure().add_subplot(projection='3d')\n",
    "    ax1.set_title(desc)\n",
    "    ax1.voxels(data > threshold)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(args)\n",
    "\n",
    "    # Load datasets.\n",
    "    train_dataset, train_dataloader, test_dataset, test_dataloader = create_datasets_and_dataloaders()\n",
    "\n",
    "    # Create the network.\n",
    "#     net = VAE3D(128)\n",
    "#     if torch.cuda.is_available():\n",
    "#         net.cuda()\n",
    "\n",
    "#     # Load a model if given.\n",
    "#     args.model = 'outputs_vae5/model_200.pth'\n",
    "#     if args.model != '':\n",
    "#         net.load_state_dict(torch.load(args.model))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        import random\n",
    "        data1, data2 = random.choice(test_dataset).unsqueeze(0), random.choice(test_dataset).unsqueeze(0)\n",
    "        data = torch.cat((data1, data2), 0)\n",
    "        visualize(data[0,0,...], desc='Data 1')\n",
    "        visualize(data[1,0,...], desc='Data 2')\n",
    "        \n",
    "        z, _ = net.encoder(blur(data.cuda()))\n",
    "        t = torch.linspace(0, 1, 11).unsqueeze(1).cuda()\n",
    "        zt = (1-t)*z[[0],:] + t*z[[1],:]\n",
    "        recon = net.decoder(zt)\n",
    "        for i in tqdm(range(11)):\n",
    "            visualize(recon[i,0,...], threshold=0.1, desc=f'Mixture (t = {t[i].item()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(args)\n",
    "\n",
    "    # Load datasets.\n",
    "    train_dataset, train_dataloader, test_dataset, test_dataloader = create_datasets_and_dataloaders()\n",
    "\n",
    "    # Create the network.\n",
    "#     net = VAE3D(128)\n",
    "#     if torch.cuda.is_available():\n",
    "#         net.cuda()\n",
    "\n",
    "#     # Load a model if given.\n",
    "#     args.model = 'outputs_vae5/model_200.pth'\n",
    "#     if args.model != '':\n",
    "#         net.load_state_dict(torch.load(args.model))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        import random\n",
    "#         data = random.choice(test_dataset).unsqueeze(0)\n",
    "#         visualize(data[0,0,...], desc='Data - original')\n",
    "        \n",
    "        BASIS = 2\n",
    "        print(f'Using {BASIS}-th basis')\n",
    "        basis = pc_basis[0,:,BASIS]\n",
    "        \n",
    "#         z, _ = net.encoder(data.cuda())\n",
    "        t = (torch.linspace(-1, 1, 7).unsqueeze(1).cuda() ** 1) * 10\n",
    "        \n",
    "        print(z, '\\n', basis)\n",
    "        deltaz = t*basis[None,:]\n",
    "        zt = deltaz\n",
    "#         zt = deltaz + z\n",
    "        recon = net.decoder(zt.float())\n",
    "        for i in tqdm(range(7)):\n",
    "#             visualize(recon[i,0,...], threshold=0.2, desc=f'Mixture (t = {t[i].item()})')\n",
    "            visualize(recon[i,0,...], threshold=0.1, desc='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-glory",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
